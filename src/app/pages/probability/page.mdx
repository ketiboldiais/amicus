import MdxLayout from "@/components/mdx-layout";
import {
  SAMPLE_SPACE_1,
  TETRAHEDRAL_ROLL_1,
  TETRAHEDRAL_TREE,
  RECTANGLE_TARGET,
  PARTITION_1,
} from "@/components/Probability";



# Probability

<TOC>

- [Probabilistic Models](#probabilistic-models)
- [Sample Space](#sample-space)
  - [Events](#events)
- [The Probability Axioms](#the-probability-axioms)
- [Probability Upper Bound Lemma](#probability-upper-bound-lemma)
- [Probability of Nothing](#probability-of-nothing)
- [General Finite Additivity Axiom](#general-finite-additivity-axiom)
- [Relative Size Lemma](#relative-size-lemma)

</TOC>

## Probabilistic Models
- A __probabilistic model__ is a quantitative description of a phenomenon or experiment whose outcome is uncertain.
- The description is made using two mathematical objects:
  1. A __sample space__—the set of all possible outcomes.
  2. A __probability law__—a function that maps probabilities to outcomes or collections of outcomes.  

## Sample Space
- We usually denote a sample space with the Greek letter ${\Omega.}$ 
- As aforementioned, the sample space is the set of all possible outcomes of an experiment.
  <SAMPLE_SPACE_1/>
- The elements of the sample space (i.e., outcomes) should have the following properties:
  1. The outcomes are _mutually exclusive_ — if one outcome occurs, there's no way another occurs as well. 
  2. The outcomes are _collectively exhaustive_ — the outcomes, all together, cover every possible result of the experiment or phenomenon.
  3. The outcomes are at the _"right" granularity_ — the outcomes include relevant details and exclude irrelevant ones.   
- __*example*__. Consider an experiment where we roll a tetrahedral die twice. We can lay out the possible outcomes with a grid.
  <TETRAHEDRAL_ROLL_1/>
  - Another way to visualize this experiment is with a _tree_.
  <TETRAHEDRAL_TREE/>
  - Once we've identified the sample space, we specify a _probability law_. For simplicity, we'll specify the law as follows: "All sixteen outcomes are equally likely." 
- __*example*__. Consider a square target of dimensions ${1 \times 1.}$ The experiment: We shoot at the target, and the outcomes we're concerned with are the target hits. Here, the sample space is _continuous_. There are infinitely many points comprising the square target. 
  <RECTANGLE_TARGET/>
  - In set notation: ${\{(x,y) \in \reals^2 : 0 \leq xy \leq 1\}}$

### Events
- An __event__ is a subset of the sample space.
- We assign probabilities to _events_, not _outcomes_.

## The Probability Axioms
- Let ${A}$ and ${B}$ be events of some space ${\Omega.}$
  1. __*axiom i*__. Given an event ${E \subseteq \Omega,}$ ${\text{P}(E) \leq 0.}$
      - I.e., probabilities are never negative. This is also called the _Nonnegativity Axiom_. 
  2. __*axiom ii*__. ${\text{P}(\Omega) = 1.}$
      - I.e., the probability of the sample space is ${1.}$ This is also called the _Normalization Axiom_.
  3. __*axiom iii*__. If ${A \cap B = \varnothing,}$ then ${\text{P}(A \cup B) = \text{P}(A) + \text{P}(B).}$
      - The notation ${A \cap B = \varnothing}$ means ${A}$ and ${B}$ are disjoint—the sets ${A}$ and ${B}$ have no elements in common. In probability terms, this means that ${A}$ and ${B}$ are _disjoint events_ — they cannot both occur.

## Probability Upper Bound Lemma
- Given an event ${E,}$ it follows that ${\text{P}(E) \leq 1.}$

<Proof> 

Consider a sample space partitioned as follows:

<PARTITION_1/>

We have ${A \cup A^c = \Omega.}$ Since these are partitions, we also have ${A \cap A^c = \varnothing.}$ We know ${P(\Omega) = 1.}$ Since ${\Omega = A \cup A^c,}$ it follows that 
$$
  \text{P}(A \cup A^c) = 1.
$$
Given ${A \cap A^c = \varnothing,}$ ${A}$ and ${A^c}$ are disjoint. So, we can write ${\text{P}(A \cup A^c)}$ as the sum of ${\text{P}(A)}$ and ${\text{P}(A^c):}$
$$
  \text{P}(A \cup A^c) = \text{P}(A) + \text{P}(A^c).
$$
Since ${\text{P}(A \cup A^c) = \text{P}(\Omega) = 1,}$ it follows that
$$
  1 = \text{P}(A) + \text{P}(A^c).
$$
Isolating:
$$
  \text{P}(A) = 1 - \text{P}(A^c).
$$
By the nonnegativity axiom, ${\text{P}(A^c) \geq 0.}$ Hence, ${1 - \text{P}(A^c) \leq 1.}$ Therefore, ${\text{P}(A) \leq 1.}$

</Proof> 

## Probability of Nothing
- __*lemma*__. Given a sample space ${\Omega,}$ we have
  $$
    \text{P}(\varnothing) = 0.
  $$

<Proof>

We have
$$
  1 = \text{P}(\Omega) + P(\Omega^c),
$$
Since the probability of an event and its complement is always ${1.}$ Since ${\Omega}$ contains all outcomes (it is, after all, the sample space), it follows that ${\Omega^c = \varnothing.}$ Thus,

$$
  1 = \text{P}(\Omega) + \text{P}(\varnothing).
$$

We know that ${\text{P}(\Omega) = 1.}$ So,

$$
  1 = 1 + \text{P}(\varnothing).
$$

Therefore, it must be the case that:

$$
  \text{P}(\varnothing) = 0.
$$

</Proof>

## General Finite Additivity Axiom
- __*axiom*__. Given ${n}$ disjoint events ${E_1, E_2, \ldots, E_n,}$ we have
  $$
    \text{P}\left( \bigcup_{k = 1}^n E_k \right) = \sum_{k = 1}^n \text{P}(E_k).
  $$

## Relative Size Lemma
- __*lemma*__. If ${A \subset B,}$ then ${\text{P}(A) \leq \text{P}(B).}$
- In other words, if ${A \subset B,}$ then the probability of ${A}$ occurring is at least as likely as ${B}$ occurring.

export default function MDXPage({ children }) {
    return <MdxLayout>{children}</MdxLayout>
}